
<html><head>
    <title>Chuang Gan</title>
    <link rel="shortcut icon" href="imgs/favicon.ico">

    <style type="text/css">
    body {
        margin-top: 30px;
        margin-bottom: 20px;
        margin-left: 0px;
        margin-right: 0px;
      width: 100%;
    }
    p {
        margin-top: 0px;
        margin-bottom: 0px;
    }

    .caption {
        font-size: 25px;
        font-weight: normal;
        color: #000;
        font-family:  Arial, sans-serif;
    }
    .caption-1 {
        font-size: 18px;
        font-family:  Arial, sans-serif;
      margin-bottom: 3px;
    }
    .caption-2 {
        font-size: 16px;
        font-family:  Arial, sans-serif;
        font-weight: bold;
      margin-bottom: 1px;
        color: #000079;
    }
    .caption-3 {
        font-size: 16px;
        font-family:  Arial, sans-serif;
        font-weight: bold;
      margin-bottom: 1px;
        color: #F00;
    }

    .caption-4 {
        font-size: 16px;
        font-family:  Arial, sans-serif;
        color: #000079;
    }
    .content {
        font-size: 16px;
        font-family:  Arial, sans-serif;
      margin-bottom: 5px;
        text-align: justify;
    }
    .content a {
        font-size: 16px;
        font-family:  Arial, sans-serif;
      margin-bottom: 10px;
        color: #000;

    }
    .content strong a {
        font-size: 16px;
      font-weight: initial;
        font-family:  Arial, sans-serif;
      margin-bottom: 10px;
        color: #000079;
    }

    .author {
      font-size: 16px;
      font-style:oblique;
      font-family:  Arial, sans-serif;
      text-align: justify;
      margin-bottom: 1px;
    }
    .author a {
      font-size: 16px;
      font-style:oblique;
      font-family:  Arial, sans-serif;
      color: #000;
      margin-bottom: 1px;
    }
    .author strong a {
      font-size: 16px;
      font-style:oblique;
      font-weight: initial;
      font-family:  Arial, sans-serif;
      color: #000079;
    }

    .title-small {
      margin-top: 20px;
      margin-bottom: 20px;
        font-size: 20px;
      font-weight: bold;
      font-family:  Arial, sans-serif;	/* font-weight: bold; */
        color: #000;
    }
    .title-large {
        font-size: 24px;
      margin-bottom: 10px;
      font-family:  Arial, sans-serif;
      font-weight: bold;
        color: #000;
    }
    .margin {
        font-size: 10px;
        line-height: 10px;
    }
    .margin-small {
        font-size: 5px;
        line-height: 5px;
    }
    .margin-large {
        font-size: 16px;
        line-height: 16px;
    }
    a:link {
        text-decoration: none;
    }
    a:visited {
        text-decoration: none;
    }
    content a:link {
        text-decoration: none;
    }
    content a:visited {
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    a:active {
        text-decoration: underline;
        color: #000000;
        font-family:  Tahoma, Geneva, sans-serif;
    }
    strong a:active {
        text-decoration: underline;
        color: #000000;
    }
    img
    {
     border-color: black;
    }

    #pubs .pubs-list {
        display: none;
    }

    #pubs.show-by-date .pubs-list.by-date {
        display: block;
    }

    #pubs.show-by-topic .pubs-list.by-topic {
        display: block;
    }

    .project-layout{
      position: relative;
      margin-top: 8px;
    }

    .project-layout .title-large{
      position: absolute;
      top:0;
      left:0;
    }

    .project {
        display: flex;
        flex-direction: row;
        align-items: center;
    }

    .project h3{
      font-size: 30px;
      font-weight: bold;
      margin-bottom: 16px;
    }

    .project p{
      font-size: 18px;
      margin-bottom: 16px;
      text-align: center;
    }

    .project .link{
      width:100%;
      line-height: 1.5;
    }

    .project .intro {
        width: 50%;
        display: flex;
        align-items: center;
        flex-direction: column;
    }

    .project .intro .more {
        /* background-color: rgb(0, 0, 238); */
        background-color: #0d47a1;
        border: none;
        color: white;
        padding: 8px 26px;
        margin: 5px;
        margin-top: 20px;
        border-radius: 5px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        transition: 0.3s;
    }

    .project .intro .more:hover{
      opacity: 0.7;
    }

    .project .preview {
      /* object-fit: none; */
      outline: none;
    }
    </style>
    <meta http-equiv="Content-Type" content="text/html; charset=gbk"></head>
    <script src="scroll.js"></script>
    <script>
      // (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      // (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      // m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      // })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      // ga('create', 'UA-53682931-1', 'auto'); // maybe a new account of google analysis is needed(https://www.google-analytics.com).
      // ga('send', 'pageview');
      function showPubs(id) {
    	var pubs = document.getElementById('pubs');
        pubs.classList.toggle('show-by-date', id === 0)
        pubs.classList.toggle('show-by-topic', id === 1)
        return true;
      }
    </script>

    <body><div style="width: 1085px; margin: auto;">

    <table border="0" width="100%" style="margin: auto">
      <tbody>

        <tr>

        <td width="185"><table>
                <tr><td>
                <a href="imgs/portrait.jpg"><img src="imgs/portrait.jpg"  height="320" alt="Chuang Gan;"></td></tr>
                </table></td>
        <td width="15"></td>
        <td></td>
        <td><table border="0" width="100%">
          <tbody>
          <tr height="10">
            <td colspan="2"></td></tr>


             <tr height="20">
            <td>
               <!-- add as more information here as you like, and this line is also an example of html annoation -->
               <p class="caption">Chuang Gan</p>

               <!-- <p class="caption">Chuang Gan&nbsp;<span class="content"><a href="mailto:ganchuang@csail.edu.com">ganchuang[AT]csail[dot]edu[dot]com</a></span></p> -->
               <!-- <p class="content"><a href="mailto:ganchuang@csail.edu.com">ganchuang[AT]csail[dot]edu[dot]com</a></p> -->
               <!-- <p class="content"><b>Computer Science and Artificial Intelligence Laboratory</b></p> -->
               <br>
            </td>
          </tr>
          <!-- <table border="0">
              <tbody> -->
                <tr>
                  <td width="700">
                <p align="justify" class="content">I am a principal research staff member and director of robotics lab at IBM Research. I am also a research scientist at MIT, working closely with Prof. <strong><a href="http://web.mit.edu/torralba/www/" target="_blank" rel="nofollow" class="caption-2">Antonio Torralba</a></strong> and Prof. <strong><a href="https://cocosci.mit.edu/josh" target="_blank" rel="nofollow" class="caption-2">Josh Tenenbaum</a></strong>.  Before that, I completed my PhD with <strong><a href="http://iiis.tsinghua.edu.cn/en/show-6602-1.html" target="_blank" rel="nofollow" class="caption-2">the highest honor</a></strong> at Tsinghua University,  where I was supervised by Prof. <strong><a href="http://iiis.tsinghua.edu.cn/yao/" target="_blank" rel="nofollow" class="caption-2">Andrew Chi-Chih Yao</a></strong>.  My research lies at the intersection of computer vision, AI, cognitive science, and robotics.  The overarching goal of my research is to build a human-like common sense machine that is capable of sensing, reasoning, and acting in the physical world.  My  works have been recognized by Microsoft Fellowship, Baidu Fellowship, and media coverage from CNN, BBC, The New York Times, WIRED, Forbes, and MIT Tech Review.</p>  
                </td>
                </tr>
              <!-- </tbody>
            </table> -->
          <tr height="5">
            <td>
              <p class="margin">&nbsp;</p>
                <p class="content">
                   <strong><a href="https://scholar.google.com/citations?user=PTeSCbIAAAAJ&hl=en">Google Scholar</a></strong> |
                  <strong><a href="#sect-links">Contact</a></strong> |
                  <strong><a href="#sect-events">News</a></strong> |
                  <strong><a href="#sect-publications">Publications</a></strong> |
                  <strong><a href="#sect-competitions">Competitions</a></strong> |
                  <strong><a href="#sect-software">Software</a></strong> |
                  <strong><a href="#sect-Honors">Honors</a></strong> |
                  <strong><a href="http://accessibility.mit.edu">Accessibility</a></strong></p>
            </td>
          </tr>
          <tr height="5">
            <td colspan="2"></td></tr>
        </tbody></table></td>
      </tr>
    </tbody></table>
    <p class="margin">&nbsp;</p>



    <br>
    <!-- contact & links -->
    <p id="sect-links" class="title-large">Contact</p>
    <strong><a href="https://dblp.org/pers/hd/g/Gan:Chuang">Email: ganchuang [at] csail (dot) mit (dot) edu </a></strong></p>
    <br>

    <!-- news here -->
    <p id="sect-events" class="title-large">News</p>

    <li> I am serving as an Area Chair for  ICLR 2022, CVPR 2022, ICML 2022, ECCV 2022, ICCV 2021, ACL 2021 and NeurIPS 2021.
    <li> Code and dataset of <a href="https://github.com/hzaskywalker/PlasticineLab">PlasticineLab</a>, <a href="https://github.com/zfchenUnique/DCL-Release">DCL</a>, and <a href="https://github.com/Ordered-Memory-RL/">Order-Memory Policy Network</a> have been released. 
    <li> Code and dataset of <a href="https://github.com/chuangg/Foley-Music">Foley Music</a> have been released. 
    <li> Code, dataset and evaluation server of <a href="http://clevrer.csail.mit.edu/">Video CLEVRER </a> have been released.
    <br>
    <!-- add more news as you like using above format -->
    <br>


    <div class="project-layout">
        <p class="title-large">Research Highlight</p>
        <div class="project">
            <div class="intro">
                <h3>ThreeDWorld (TDW)</h3>
                <p> A Multi-Modal Interactive Physical Simulation Platform for<br /> Computer Vision, Robotics and Cognitive Science</p>
                <a class="link" href="http://tdw-transport.csail.mit.edu/">Transport Challenge for Visually Guided Task and Motion Planing</a>
                <a class="link" href="https://www.tshu.io/AGENT/">Agent Benchmark for Psychological Reasoning</a>
                <a href="http://www.threedworld.org/" class="more">ThreeDWorld Website</a>
            </div>
            <!--  -->
            <video width="540" height="303.75" preload poster="imgs/video-poster.jpg" autoplay loop controls class="preview">
                <source src="http://www.threedworld.org/img/Intro_reel_updated_cups_V1.mp4" type="video/mp4">
            </video>
        </div>
    </div>
    <br>

    <p id="sect-publications" class="title-large">Publications(<a href="#sect-publications" onclick="showPubs(0); return false;">by date</a> / <a href="#sect-publications" onclick="showPubs(1)">by topic</a>)</p>
    <div id="pubs" class="show-by-date">
    <div class='pubs-list by-date'>



    <p class="title-small">2022</p>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/tdw-transport.png"><img src="imgs/tdw-transport.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://tdw-transport.csail.mit.edu/"> The ThreeDWorld Transport Challenge: A Visually Guided Task-and-Motion Planning Benchmark Towards Physically Realistic Embodied AI
 </a> </strong></p>
          <p class="author"> <strong>Chuang Gan</strong>, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar, Dan Gutfreund, Daniel L.K. Yamins, James J DiCarlo, Josh McDermott, Antonio Torralba, Joshua B. Tenenbaum
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICRA 2022</strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/RISP.gif"><img src="imgs/RISP.gif" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://sites.google.com/view/risp-iclr-2022"> RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation
 </a> </strong></p>
          <p class="author"> Pingchuan Ma*, Tao Du*, Joshua B. Tenenbaum, Wojciech Matusik, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2022 (Oral)</strong>
          </tr>
    </tbody></table>
    </tbody></table>

        <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/Diffskill.gif"><img src="imgs/Diffskill.gif" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://xingyu-lin.github.io/diffskill/">
            DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools

 </a> </strong></p>
          <p class="author">  Xingyu Lin, Zhiao Huang, Yunzhu Li, Joshua B. Tenenbaum,  David Held, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2022 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

  <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/emergent.jpg"><img src="imgs/emergent.jpg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://openreview.net/pdf?id=49A1Y6tRhaq"> Linking Emergent and Natural Languages via Corpus Transfer </a> </strong></p>
          <p class="author">  Shunyu Yao, Mo Yu, Yang Zhang, Karthik R Narasimhan, Joshua B. Tenenbaum, <strong>Chuang Gan</strong> </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2022 (Spotlight) </strong>
          </tr>
    </tbody></table>
    </tbody></table>

      <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/Comphy.gif"><img src="imgs/Comphy.gif" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://comphyreasoning.github.io/"> ComPhy: Compositional Physical Reasoning of Objects and Events from Videos </a> </strong></p>
          <p class="author">  Zhenfang Chen, Kexin Yi, Yunzhu Li, Mingyu Ding, Antonio Torralba, Joshua B. Tenenbaum, <strong>Chuang Gan</strong></p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2022 </strong>
          </tr>
    </tbody></table>
    </tbody></table>


        <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/CPDeform.png"><img src="imgs/CPDeform.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://sites.google.com/view/cpdeformiclr2022"> 
            Contact Points Discovery for Soft-Body Manipulations with Differentiable Physics
 </a> </strong></p>
          <p class="author">  Sizhe Li*, Zhiao Huang*, Tao Du, Hao Su, Joshua B. Tenenbaum, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2022 (Spotlight) </strong>
          </tr>
    </tbody></table>
    </tbody></table>





      <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/FALCON.png"><img src="imgs/FALCON.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://openreview.net/pdf?id=htWIlvDcY8"> 
FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations </a> </strong></p>
          <p class="author">  Lingjie Mei*, Jiayuan Mao*, Ziqi Wang, <strong>Chuang Gan</strong>, Joshua B. Tenenbaum </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2022 </strong>
          </tr>
    </tbody></table>
    </tbody></table>




    <p class="title-small">2021</p>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/TDW.jpeg"><img src="imgs/TDW.jpeg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/2007.04954.pdf"> ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation </a> </strong></p>
          <p class="author">   <strong>Chuang Gan</strong>, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, Kuno Kim, Elias Wang, Michael Lingelbach, Aidan Curtis, Kevin Tyler Feigelis, Daniel Bear, Dan Gutfreund, David Daniel Cox, Antonio Torralba, James J. DiCarlo, Joshua B. Tenenbaum, Josh Mcdermott, Daniel LK Yamins
    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS Dataset 2021 (Oral) </strong>
          </tr>
    </tbody></table>
    </tbody></table>

<table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/teaser_small.gif"><img src="imgs/teaser_small.gif" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://vrdp.csail.mit.edu/"> Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language
 </a> </strong></p>
          <p class="author">  Mingyu Ding, Zhenfang Chen, Tao Du, Ping Luo, Joshua B. Tenenbaum, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

<table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/PTR.png"><img src="imgs/PTR.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://ptr.csail.mit.edu/"> PTR: A Benchmark for Part-based Conceptual, Relational, and Physical Reasoning</a> </strong></p>
          <p class="author">  Yining Hong, Li Yi, Joshua B. Tenenbaum, Antonio Torralba, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

  <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/Contrastive.png"><img src="imgs/Contrastive.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/2111.01124.pdf">When Does Contrastive Learning Preserve Adversarial Robustness from Pretraining to Finetuning? </a> </strong></p>
          <p class="author">  Lijie Fan, Sijia Liu, Pin-Yu Chen, Gaoyuan Zhang, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

  <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/Star.png"><img src="imgs/Star.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://star.csail.mit.edu/">
 STAR: A Benchmark for Situated Reasoning in Real-World Videos </a> </strong></p>
          <p class="author">  Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B. Tenenbaum, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS Dataset 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/crl.png"><img src="imgs/crl.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://yilundu.github.io/crl/"> Curious Representation Learning for Embodied Intelligence
</a> </strong></p>
          <p class="author"> Yilun Du, <strong>Chuang Gan</strong>, Phillip Isola </p>

          <p class="content">
          <p class="caption-2"> <strong> ICCV 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/OPEn.jpeg"><img src="imgs/OPEn.jpeg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://open.csail.mit.edu/"> OPEn: An Open-ended Physics Environment for Learning Without a Task</a> </strong></p>
          <p class="author">    <strong>Chuang Gan</strong>, Abhishek Bhandwaldar, Antonio Torralba, Joshua B. Tenenbaum, Phillip Isola
    </p>

          <p class="content">
          <p class="caption-2"> <strong> IROS 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>





    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/Agent.jpeg"><img src="imgs/Agent.jpeg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://www.tshu.io/AGENT"> AGENT: A Benchmark for Core Psychological Reasoning

</a> </strong></p>
          <p class="author"> Tianmin Shu, Abhishek Bhandwaldar, <strong>Chuang Gan</strong>, Kevin A. Smith,
Shari Liu, Dan Gutfreund, Elizabeth Spelke, Joshua B. Tenenbaum, Tomer D. Ullman </p>

          <p class="content">
          <p class="caption-2"> <strong> ICML 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/toqnet.jpeg"><img src="imgs/toqnet.jpeg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://toqnet.csail.mit.edu/">Temporal and Object Quantification Networks</a> </strong></p>
          <p class="author"> Jiayuan Mao, Zhezheng Luo, <strong>Chuang Gan</strong>, Joshua B. Tenenbaum, Jiajun Wu, Leslie P. Kaelbling, Tomer D. Ullman </p>

          <p class="content">
          <p class="caption-2"> <strong> IJCAI 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

  <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/plasticine.jpeg"><img src="imgs/plasticine.jpeg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://plasticinelab.csail.mit.edu/"> PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable Physics.</a> </strong></p>
          <p class="author"> Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B. Tenenbaum, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2021 (Spotlight) </strong>
          </tr>
    </tbody></table>
    </tbody></table>


    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/DCL.gif"><img src="imgs/DCL.gif" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://dcl.csail.mit.edu/"> Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning</a> </strong></p>
          <p class="author">  Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua B. Tenenbaum, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>


    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/OMN.jpg"><img src="imgs/OMN.jpg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://ordered-memory-rl.github.io/"> Learning Task Decomposition with Order-Memory Policy Network </a> </strong></p>
          <p class="author">  Yuchen Lu, Yikang Shen, Siyuan Zhou, Aaron Courville, Joshua B. Tenenbaum, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>    

    <p class="title-small">2020</p>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/foley_music.png"><img src="imgs/foley_music.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://foley-music.csail.mit.edu/"> Foley Music: Learning to Generate Music from Videos</a> </strong></p>
          <p class="author">  <strong>Chuang Gan</strong>, Deng Huang, Peihao Chen, Joshua B. Tenenbaum, Antonio Torralba
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ECCV 2020 </strong>
          </tr>
    </tbody></table>
    </tbody></table>


    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/music-gesture.jpg"><img src="imgs/music-gesture.jpg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://music-gesture.csail.mit.edu/"> Music Gesture for Visual Sound Separation</a> </strong></p>
          <p class="author">  <strong>Chuang Gan</strong>, Deng Huang, Hang Zhao, Joshua B. Tenenbaum, Antonio Torralba
    </p>

          <p class="content">
          <p class="caption-2"> <strong> CVPR 2020 </strong>
          </tr>
    </tbody></table>
    </tbody></table>


    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/video_grounding.jpeg"><img src="imgs/video_grounding.jpeg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/abs/2004.03545"> Dense Regression Network For Video Grounding</a> </strong></p>
          <p class="author">  Runhao Zeng, Haoming Xu, Wenbing Huang, Peihao Chen, Mingkui Tan, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> CVPR 2020 </strong>
          </tr>
    </tbody></table>
    </tbody></table>


   <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/video_grounding.jpeg"><img src="imgs/TinyTl.jpeg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://hanlab.mit.edu/projects/tinyml/tinyTL"> TinyTL: Reduce Activations, Not Trainable Parameters for Efficient On-Device Learning
</a> </strong></p>
          <p class="author">  Han Cai, <strong>Chuang Gan</strong>, Ligeng Zhu, Song Han
    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2020 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

     <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/MCUNET.png"><img src="imgs/MCUNET.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://mcunet.mit.edu/"> MCUNet: Tiny Deep Learning on IoT Devices

</a> </strong></p>
          <p class="author">  Ji Lin, Wei-Ming Chen, Yujun Lin, John Cohn, <strong>Chuang Gan</strong>, Song Han
    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2020 (Spotlight)</strong>
          </tr>
    </tbody></table>
    </tbody></table>



    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/mgs/clevrer.gif"><img src="imgs/clevrer.gif" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://clevrer.csail.mit.edu/">  CLEVRER: CoLlision Events for Video REpresentation and Reasoning</a> </strong></p>
          <p class="author"> Kexin Yi*, <strong>Chuang Gan*</strong>, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, Joshua B. Tenenbaum
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2020 (Spotlight) </strong>
          </tr>
    </tbody></table>
    </tbody></table>


    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/dap.jpg"><img src="imgs/dap.jpg" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://openreview.net/pdf?id=rygjHxrYDB">  Deep Audio Priors Emerge From Harmonic Convolutional Networks </a> </strong></p>
          <p class="author"> Zhoutong Zhang, Yunyun Wang, <strong>Chuang Gan</strong>, Jiajun Wu, Joshua B. Tenenbaum, Antonio Torralba, William T. Freeman</p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2020 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/ofa.png"><img src="imgs/ofa.png" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://openreview.net/pdf?id=HylxE1HKwS">  Once for All: Train One Network and Specialize it for Efficient Deployment </a> </strong></p>
          <p class="author"> Han Cai, <strong>Chuang Gan</strong>, Tianzhe Wang, Zhekai Zhang, Song Han</p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2020 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/navigation.png"><img src="imgs/navigation.png" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://avn.csail.mit.edu/">  Look, Listen, and Act: Towards Audio-Visual Embodied Navigation </a> </strong></p>
          <p class="author"> <strong>Chuang Gan*</strong>, Yiwei Zhang*, Jiajun Wu, Boqing Gong, Joshua B. Tenenbaum
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICRA 2020 </strong>
          </tr>
    </tbody></table>
    </tbody></table>


    <p class="title-small">2019</p>



    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/stereo_sound.jpeg"><img src="imgs/stereo_sound.jpeg" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://sound-track.csail.mit.edu/">  Self-supervised Moving Vehicle Tracking with Stereo Sound</a> </strong></p>
          <p class="author"> <strong>Chuang Gan</strong>, Hang Zhao, Peihao Chen, David Cox, Antonio Torralba</p>

          <p class="content">
          <p class="caption-2"> <strong> ICCV 2019 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/sound_of_motions.png"><img src="imgs/sound_of_motions.png" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/1904.05979.pdf">  The Sound of Motions</a> </strong></p>
          <p class="author"> Hang Zhao, <strong>Chuang Gan</strong>, Wei-Chiu Ma, Antonio Torralba</p>

          <p class="content">
          <p class="caption-2"> <strong> ICCV 2019 </strong>
          </tr>
    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/tsm.png"><img src="imgs/tsm.png" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/1811.08383.pdf"> TSM: Temporal Shift Module for Efficient Video Understanding</a> </strong></p>
          <p class="author"> Ji Lin, <strong>Chuang Gan</strong>, Song Han </p>

          <p class="content">
          <p class="caption-2"> <strong> ICCV 2019  </strong>
          </tr>
    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/gcn_action.jpeg"><img src="imgs/gcn_action.jpeg" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/1909.03252.pdf"> Graph Convolutional Networks for Temporal Action Localization</a> </strong></p>
          <p class="author"> Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong, Peilin Zhao, Junzhou Huang, <strong>Chuang Gan</strong> </p>

          <p class="content">
          <p class="caption-2"> <strong> ICCV 2019  </strong>
          </tr>

    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/Gailfo.png"><img src="imgs/Gailfo.png" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://papers.nips.cc/paper/8317-imitation-learning-from-observations-by-minimizing-inverse-dynamics-disagreement.pdf"> Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement</a> </strong></p>
          <p class="author"> Chao Yang, Xiaojian Ma, Wenbing Huang, Fuchun Sun, Huaping Liu, Junzhou Huang, <strong>Chuang Gan</strong> </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2019 (Spotlight) </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/meta_concept.jpg"><img src="imgs/meta_concept.jpg"width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://papers.nips.cc/paper/8745-visual-concept-metaconcept-learning.pdf"> Visual Concept-Metaconcept Learning</a> </strong></p>
          <p class="author"> Chi Han, Jiayuan Mao, <strong>Chuang Gan</strong>, Josh Tenenbaum, Jiajun Wu
    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2019 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/ncn.png"><img src="imgs/ncn.png" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://papers.nips.cc/paper/8411-cross-channel-communication-networks.pdf"> Cross-channel Communication Networks </a> </strong></p>
          <p class="author"> Jianwei Yang, Zhile Ren, <strong>Chuang Gan</strong>, Hongyuan Zhu, Devi Parikh

    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2019 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/nscl.jpg"><img src="imgs/nscl.jpg" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/1904.12584.pdf"> The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision</a> </strong></p>
          <p class="author"> Jiayuan Mao, <strong>Chuang Gan</strong>, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun Wu </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2019 (Oral) </strong>
          </tr>
    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/DQ.png"><img src="imgs/DQ.png" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/1904.08444.pdf"> Defensive quantization: When efficiency meets robustness</a> </strong></p>
          <p class="author"> Ji Lin, <strong>Chuang Gan</strong>, Song Han</p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2019 </strong>
          </tr>
    </tbody></table>
    </tbody></table>




    <p class="title-small">2018</p>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/dense_caption.png"><img src="imgs/dense_caption.png" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://papers.nips.cc/paper/7569-weakly-supervised-dense-event-captioning-in-videos.pdf">  Weakly Supervised Dense Event Captioning in Videos</a> </strong></p>
          <p class="author"> Xuguang Duan, Wenbing Huang, <strong>Chuang Gan</strong>, Jingdong Wang, Wenwu Zhu, Junzhou Huang</p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2018 </strong>
          </tr>
    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/nsvqa.jpg"><img src="imgs/nsvqa.jpg" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://papers.nips.cc/paper/7381-neural-symbolic-vqa-disentangling-reasoning-from-vision-and-language-understanding.pdf">  Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding</a> </strong></p>
          <p class="author"> Kexin Yi, Jiajun Wu, <strong>Chuang Gan</strong>, Antonio Torralba, Pushmeet Kohli, Joshua B. Tenenbaum </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2018 (Spotlight) </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/sound_of_pixels.png"><img src="imgs/sound_of_pixels.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/1804.03160.pdf">  The Sound of Pixels</a> </strong></p>
          <p class="author"> Hang Zhao, <strong>Chuang Gan</strong>, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, Antonio Torralba</p>

          <p class="content">
          <p class="caption-2"> <strong> ECCV 2018  </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/keypoint.jpg"><img src="imgs/keypoint.jpg" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/1712.05765.pdf"> Unsupervised Domain Adaptation for 3D Keypoint Estimation via View Consistency </a> </strong></p>
          <p class="author"> Xingyi Zhou, Arjun Karpur, <strong>Chuang Gan</strong>, Linjie Luo, Qixing Huang</p>

          <p class="content">
          <p class="caption-2"> <strong> ECCV 2018  </strong>
          </tr>
    </tbody></table>
    </tbody></table>


    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/geometry.png"><img src="imgs/geometry.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
                <p class="caption-1 "><strong><a href="pdfs/Video_Geometry.pdf">Geometry-Guided CNNs for Self-supervised Video Representation Learning</a></strong>

          <p class="author"><strong>Chuang Gan</strong>, Boqing Gong, Kun Liu, Hao Su, Leonidas Guibas</p>

                <p class="content">
          <p class="caption-2"> <strong> CVPR 2018  </strong>
          </tr>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/attention.png"><img src="imgs/attention.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Long_Attention_Clusters_Purely_CVPR_2018_paper.pdf">Attention Clusters: Purely Attention Based Local Feature Integration for Video Classification</a></strong>

          <p class="author"> Xiang Long, <strong>Chuang Gan</strong>, Gerard de Melo, Jiajun Wu, Xiao Liu, Shilei Wen</p>


          <p class="content">
          <p class="caption-2"> <strong> CVPR 2018  </strong>
          </tr>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/tvnet.png"><img src="imgs/tvnet.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
        <p class="caption-1 "><strong><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_End-to-End_Learning_of_CVPR_2018_paper.pdf">End-to-End Learning of Motion Representation for Video Understanding</a></strong>
        <p class="author"> Lijie Fan, Wenbing Huang, <strong>Chuang Gan</strong>, Stefano Ermon, Boqing Gong, Junzhou Huang</p>
          <p class="content">
          <p class="caption-2"> <strong> CVPR 2018 (Spotlight) </strong>

          </tr>
    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/contour.png"><img src="imgs/contour.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Dekel_Sparse_Smart_Contours_CVPR_2018_paper.pdf"> Sparse, Smart Contours to Represent and Edit Images </a></strong>
          <p class="author">  Tali Dekel, <strong>Chuang Gan</strong>, Dilip Krishnan, Ce Liu, William T. Freeman</p>


          <p class="content">
          <p class="caption-2"> <strong> CVPR 2018 </strong>

          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/fated.png"><img src="imgs/fated.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "> <strong><a href="http://iiis.tsinghua.edu.cn/~weblt/papers/captioning-multi-faceted-attention.pdf"> Video Captioning with Multi-Faceted Attention</a> </strong></p>
          <p class="author">  Xiang Long, <strong>Chuang Gan</strong>, Gerard de Melo</p>
          <p class="content">
          <p class="caption-2"> <strong> TACL 2018 </strong>

          </tr>
    </tbody></table>



    <hr>
    <p class="title-small">2017</p>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/stylenet.jpg"><img src="imgs/stylenet.jpg" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Gan_StyleNet_Generating_Attractive_CVPR_2017_paper.pdf"> StyleNet: Generating Attractive Visual Captions with Styles</a> </strong></p>
          <p class="author">  <strong>Chuang Gan</strong>, Zhe Gan, Xiaodong He, Jianfeng Gao, Li Deng</p>
          <p class="content">
          <p class="caption-2"> <strong> CVPR 2017 </strong>

          </tr>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/SCN.png"><img src="imgs/SCN.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "> <strong><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Gan_Semantic_Compositional_Networks_CVPR_2017_paper.pdf"> Semantic Compositional Networks for Visual Captioning </a> </strong></p>
          <p class="author"> Zhe Gan, <strong>Chuang Gan</strong>, Xiaodong He, Yunchen Pu, Kenneth Tran, Jianfeng Gao, Lawrence Carin, Li Deng</p>
          <p class="content">
          <p class="caption-2"> <strong> CVPR 2017 (Spotlight) </strong>
          </tr>
    </tbody></table>
    </tbody></table>
    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/VQS.png"><img src="imgs/VQS.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Gan_VQS_Linking_Segmentations_ICCV_2017_paper.pdf"> VQS: Linking Segmentations to Questions and Answers for Supervised Attention in VQA and Question-Focused Semantic Segmentation</a> </strong></p>
          <p class="author"> <strong>Chuang Gan</strong>, Yandong Li, Haoxiang Li, Chen Sun, Boqing Gong</p>
          <p class="content">
          <p class="caption-2"> <strong> ICCV 2017 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/paragraph.png"><img src="imgs/paragraph.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Liang_Recurrent_Topic-Transition_GAN_ICCV_2017_paper.pdf">  Recurrent Topic-Transition GAN for Visual Paragraph Generation </a> </strong></p>
          <p class="author">  Xiaodan Liang, Zhiting Hu, Hao Zhang, <strong>Chuang Gan</strong>, Eric P. Xing</p>
          <p class="content">
          <p class="caption-2"> <strong> ICCV 2017 </strong>
          </tr>
    </tbody></table>
    <hr>

      <p class="title-small">2016</p>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/attribute.png"><img src="imgs/attribute.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gan_Learning_Attributes_Equals_CVPR_2016_paper.pdf"> Learning Attributes Equals Multi-Source Domain Generalization</a> </strong></p>
          <p class="author">  <strong>Chuang Gan</strong>, Tianbao Yang, Boqing Gong</p>
          <p class="content">
          <p class="caption-2"> <strong> CVPR 2016 (Spotlight)</strong>

          </tr>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/lead_exceed.png"><img src="imgs/lead_exceed.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gan_You_Lead_We_CVPR_2016_paper.pdf">You Lead, We Exceed: Labor-Free Video Concept Learning by Jointly Exploiting Web Videos and Images</a> </strong></p>
          <p class="author"> <strong>Chuang Gan</strong>, Ting Yao, Kuiyuan Yang, Yi Yang, Tao Mei</p>
          <p class="content">
          <p class="caption-2"> <strong> CVPR 2016 (Spotlight) </strong>
          </tr>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/zero-shot.png"><img src="imgs/zero-shot.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://rdcu.be/6su7"> Recognizing an Action Using Its Name: A Knowledge-Based Approach </a> </strong></p>
          <p class="author">  <strong>Chuang Gan</strong>, Yi Yang, Linchao Zhu, Deli Zhao, Yueting Zhuang</p>
          <p class="content">
          <p class="caption-2"> <strong> IJCV 2016 </strong>
          </tr>
          </tr>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/webly.png"><img src="imgs/webly.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://link.springer.com/chapter/10.1007/978-3-319-46487-9_52">Webly-Supervised Video Recognition by Mutually Voting for Relevant Web Images and Web Video Frames</a> </strong></p>
          <p class="author"> <strong>Chuang Gan</strong>, Chen Sun, Lixin Duan, Boqing Gong</p>
          <p class="content">
          <p class="caption-2"> <strong> ECCV 2016 </strong>

          </tr>
    </tbody></table>
    </tbody></table>

          </tr>




    <hr>
      <p class="title-small">2015</p>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/devnet.png"><img src="imgs/devnet.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "> <strong><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gan_DevNet_A_Deep_2015_CVPR_paper.pdf"> DevNet: A Deep Event Network for multimedia event detection and evidence recounting </a> </strong></p>
          <p class="author">  <strong>Chuang Gan</strong>, Naiyan Wang, Yi Yang, Dit-Yan Yeung, Alexander G. Hauptmann</p>


          <p class="content">
          <p class="caption-2"> <strong> CVPR 2015 </strong>


          </tr>
    </tbody></table>
    </tbody></table>
    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/ACD.png"><img src="imgs/ACD.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/abs/1509.07225">Automatic Concept Discovery from Parallel Text and Visual Corpora</a> </strong></p>
          <p class="author"> Chen Sun, <strong>Chuang Gan</strong>, Ram Nevatia</p>
          <p class="content">
          <p class="caption-2"> <strong> ICCV 2015 </strong>

          </tr>
    </tbody></table>
    </tbody></table>




      <!-- add more papers using above format -->
    </div>

    <div class="pubs-list by-topic">



    <p class="title-small">Embodied Intelligence</p>

 <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/TDW.jpeg"><img src="imgs/TDW.jpeg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/2007.04954.pdf"> ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation </a> </strong></p>
          <p class="author">   <strong>Chuang Gan</strong>, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, Kuno Kim, Elias Wang, Michael Lingelbach, Aidan Curtis, Kevin Tyler Feigelis, Daniel Bear, Dan Gutfreund, David Daniel Cox, Antonio Torralba, James J. DiCarlo, Joshua B. Tenenbaum, Josh Mcdermott, Daniel LK Yamins
    </p>

          <p class="content">
          <p class="caption-2"> <strong>  NeurIPS Dataset 2021 (Oral) </strong>
          </tr>
    </tbody></table>
    </tbody></table>

        <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/tdw-transport.png"><img src="imgs/tdw-transport.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://tdw-transport.csail.mit.edu/"> The ThreeDWorld Transport Challenge: A Visually Guided Task-and-Motion Planning Benchmark Towards Physically Realistic Embodied AI
 </a> </strong></p>
          <p class="author"> <strong>Chuang Gan</strong>, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar, Dan Gutfreund, Daniel L.K. Yamins, James J DiCarlo, Josh McDermott, Antonio Torralba, Joshua B. Tenenbaum
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICRA 2022</strong>
          </tr>
    </tbody></table>
    </tbody></table>


     <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/RISP.gif"><img src="imgs/RISP.gif" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://sites.google.com/view/risp-iclr-2022"> RISP: Rendering-Invariant State Predictor with Differentiable Simulation and Rendering for Cross-Domain Parameter Estimation
 </a> </strong></p>
          <p class="author"> Pingchuan Ma*, Tao Du*, Joshua B. Tenenbaum, Wojciech Matusik, <strong>Chuang Gan (Oral) </strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2022 (Oral) </strong>
          </tr>
    </tbody></table>
    </tbody></table>

        <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/Diffskill.gif"><img src="imgs/Diffskill.gif" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://sites.google.com/view/iclr2022diffskill"> 
            DiffSkill: Skill Abstraction from Differentiable Physics for Deformable Object Manipulations with Tools

 </a> </strong></p>
          <p class="author">  Xingyu Lin, Zhiao Huang, Yunzhu Li, Joshua B. Tenenbaum,  David Held, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2022 </strong>
          </tr>
    </tbody></table>
    </tbody></table>
        <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/CPDeform.png"><img src="imgs/CPDeform.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://sites.google.com/view/cpdeformiclr2022"> 
            Contact Points Discovery for Soft-Body Manipulations with Differentiable Physics
 </a> </strong></p>
          <p class="author">  Sizhe Li*, Zhiao Huang*, Tao Du, Hao Su, Joshua B. Tenenbaum, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2022 (Spotlight) </strong>
          </tr>
    </tbody></table>
    </tbody></table>


    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/OPEn.jpeg"><img src="imgs/OPEn.jpeg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://open.csail.mit.edu/"> OPEn: An Open-ended Physics Environment for Learning Without a Task</a> </strong></p>
          <p class="author">    <strong>Chuang Gan</strong>, Abhishek Bhandwaldar, Antonio Torralba, Joshua B. Tenenbaum, Phillip Isola
    </p>

          <p class="content">
          <p class="caption-2"> <strong> IROS 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>



    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/crl.png"><img src="imgs/crl.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://yilundu.github.io/crl/"> Curious Representation Learning for Embodied Intelligence
</a> </strong></p>
          <p class="author"> Yilun Du, <strong>Chuang Gan</strong>, Phillip Isola </p>

          <p class="content">
          <p class="caption-2"> <strong> ICCV 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

<table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/plasticine.jpeg"><img src="imgs/plasticine.jpeg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://plasticinelab.csail.mit.edu/"> PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable Physics.</a> </strong></p>
          <p class="author">   Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B. Tenenbaum, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2021 (Spotlight) </strong>
          </tr>
    </tbody></table>
    </tbody></table>
   <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/OMN.jpg"><img src="imgs/OMN.jpg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://openreview.net/pdf?id=vcopnwZ7bC"> Learning Task Decomposition with Order-Memory Policy Network. </a> </strong></p>
          <p class="author">  Yuchen Lu, Yikang Shen, Siyuan Zhou, Aaron Courville, Joshua B. Tenenbaum, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>    

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/navigation.png"><img src="imgs/navigation.png" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/1912.11684.pdf">  Look, Listen, and Act: Towards Audio-Visual Embodied Navigation </a> </strong></p>
          <p class="author"> <strong>Chuang Gan*</strong>, Yiwei Zhang*, Jiajun Wu, Boqing Gong, Joshua B. Tenenbaum
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICRA 2020 </strong>
          </tr>
    </tbody></table>
    </tbody></table>


     <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/Gailfo.png"><img src="imgs/Gailfo.png" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://papers.nips.cc/paper/8317-imitation-learning-from-observations-by-minimizing-inverse-dynamics-disagreement.pdf"> Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement</a> </strong></p>
          <p class="author"> Chao Yang, Xiaojian Ma, Wenbing Huang, Fuchun Sun, Huaping Liu, Junzhou Huang, <strong>Chuang Gan</strong> </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2019 (Spotlight) </strong>
          </tr>
    </tbody></table>
    </tbody></table>

 <hr>

    <p class="title-small">Audio-Visual Scene Analysis</p>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/foley_music.png"><img src="imgs/foley_music.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://foley-music.csail.mit.edu/"> Foley Music: Learning to Generate Music from Videos</a> </strong></p>
          <p class="author">  <strong>Chuang Gan</strong>, Deng Huang, Peihao Chen, Joshua B. Tenenbaum, Antonio Torralba
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ECCV 2020 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/music-gesture.jpg"><img src="imgs/music-gesture.jpg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://music-gesture.csail.mit.edu/"> Music Gesture for Visual Sound Separation</a> </strong></p>
          <p class="author">  <strong>Chuang Gan</strong>, Deng Huang, Hang Zhao, Joshua B. Tenenbaum, Antonio Torralba
    </p>

          <p class="content">
          <p class="caption-2"> <strong> CVPR 2020 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="mgs/sound_of_pixels.png"><img src="imgs/sound_of_pixels.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="hhttps://arxiv.org/pdf/1504.03160.pdf">  The Sound of Pixels</a> </strong></p>
          <p class="author"> Hang Zhao, <strong>Chuang Gan</strong>, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, Antonio Torralba</p>

          <p class="content">
          <p class="caption-2"> <strong> ECCV 2018  </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/stereo_sound.jpeg"><img src="imgs/stereo_sound.jpeg" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://people.csail.mit.edu/ganchuang/">  Self-supervised Moving Vehicle Tracking with Stereo Sound</a> </strong></p>
          <p class="author"> <strong>Chuang Gan</strong>, Hang Zhao, Peihao Chen, David Cox, Antonio Torralba</p>

          <p class="content">
          <p class="caption-2"> <strong> ICCV 2019 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/sound_of_motions.png"><img src="imgs/sound_of_motions.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/1904.05979.pdf">  The Sound of Motions</a> </strong></p>
          <p class="author"> Hang Zhao, <strong>Chuang Gan</strong>, Wei-Chiu Ma, Antonio Torralba</p>

          <p class="content">
          <p class="caption-2"> <strong> ICCV 2019 </strong>
          </tr>

    </tbody></table>
    </tbody></table>




    <hr>
    <p class="title-small">Visual Commonsense  Reasoning</p>




      <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/Comphy.gif"><img src="imgs/Comphy.gif" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://comphyreasoning.github.io/"> ComPhy: Compositional Physical Reasoning of Objects and Events from Videos </a> </strong></p>
          <p class="author">  Zhenfang Chen, Kexin Yi, Yunzhu Li, Mingyu Ding, Antonio Torralba, Joshua B. Tenenbaum, <strong>Chuang Gan</strong></p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2022 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

        <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/emergent.jpg"><img src="imgs/emergent.jpg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://openreview.net/pdf?id=49A1Y6tRhaq"> Linking Emergent and Natural Languages via Corpus Transfer </a> </strong></p>
          <p class="author">  Shunyu Yao, Mo Yu, Yang Zhang, Karthik R Narasimhan, Joshua B. Tenenbaum, <strong>Chuang Gan</strong> </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2022 (Spotlight)</strong>
          </tr>
    </tbody></table>
    </tbody></table>

      <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/FALCON.png"><img src="imgs/FALCON.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://openreview.net/pdf?id=htWIlvDcY8"> 
FALCON: Fast Visual Concept Learning by Integrating Images, Linguistic descriptions, and Conceptual Relations </a> </strong></p>
          <p class="author">  Lingjie Mei*, Jiayuan Mao*, Ziqi Wang, <strong>Chuang Gan</strong>, Joshua B. Tenenbaum </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2022 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/teaser_small.gif"><img src="imgs/teaser_small.gif" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://vrdp.csail.mit.edu/"> Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language
 </a> </strong></p>
          <p class="author">  Mingyu Ding, Zhenfang Chen, Tao Du, Ping Luo, Joshua B. Tenenbaum, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

<table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/PTR.png"><img src="imgs/PTR.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://ptr.csail.mit.edu/"> PTR: A Benchmark for Part-based Conceptual, Relational, and Physical Reasoning</a> </strong></p>
          <p class="author">  Yining Hong, Li Yi, Joshua B. Tenenbaum, Antonio Torralba, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

     <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/Star.png"><img src="imgs/Star.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://star.csail.mit.edu/">
 STAR: A Benchmark for Situated Reasoning in Real-World Videos </a> </strong></p>
          <p class="author">  Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B. Tenenbaum, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS Dataset 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>



        <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/Agent.jpeg"><img src="imgs/Agent.jpeg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://www.tshu.io/AGENT"> AGENT: A Benchmark for Core Psychological Reasoning

</a> </strong></p>
          <p class="author"> Tianmin Shu, Abhishek Bhandwaldar, <strong>Chuang Gan</strong>, Kevin A. Smith,
Shari Liu, Dan Gutfreund, Elizabeth Spelke, Joshua B. Tenenbaum, Tomer D. Ullman </p>

          <p class="content">
          <p class="caption-2"> <strong> ICML 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/toqnet.jpeg"><img src="imgs/toqnet.jpeg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://toqnet.csail.mit.edu/">Temporal and Object Quantification Networks</a> </strong></p>
          <p class="author"> Jiayuan Mao, Zhezheng Luo, <strong>Chuang Gan</strong>, Joshua B. Tenenbaum, Jiajun Wu, Leslie P. Kaelbling, Tomer D. Ullman </p>

          <p class="content">
          <p class="caption-2"> <strong> IJCAI 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/DCL.gif"><img src="imgs/DCL.gif" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://openreview.net/pdf?id=bhCDO_cEGCz"> Grounding Physical Object and Event Concepts Through Dynamic Visual Reasoning.</a> </strong></p>
          <p class="author">  Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua B. Tenenbaum, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>


    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/mgs/clevrer.gif"><img src="imgs/clevrer.gif" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://clevrer.csail.mit.edu/">  CLEVRER: CoLlision Events for Video REpresentation and Reasoning</a> </strong></p>
          <p class="author"> Kexin Yi*, <strong>Chuang Gan*</strong>, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, Joshua B. Tenenbaum
    </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2020 (Oral Spotlight) </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/video_grounding.jpeg"><img src="imgs/video_grounding.jpeg" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/abs/2004.03545"> Dense Regression Network For Video Grounding</a> </strong></p>
          <p class="author">  Runhao Zeng, Haoming Xu, Wenbing Huang, Peihao Chen, Mingkui Tan, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> CVPR 2020 </strong>
          </tr>
    </tbody></table>




    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/nscl.jpg"><img src="imgs/nscl.jpg" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/1904.12584.pdf"> The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision</a> </strong></p>
          <p class="author"> Jiayuan Mao, <strong>Chuang Gan</strong>, Pushmeet Kohli, Joshua B. Tenenbaum, Jiajun Wu </p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2019 (Oral) </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/meta_concept.jpg"><img src="imgs/meta_concept.jpg"width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://papers.nips.cc/paper/8745-visual-concept-metaconcept-learning.pdf"> Visual Concept-Metaconcept Learning</a> </strong></p>
          <p class="author"> Chi Han, Jiayuan Mao, <strong>Chuang Gan</strong>, Josh Tenenbaum, Jiajun Wu
    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2019 </strong>
          </tr>
    </tbody></table>
    </tbody></table>


    <table border="0">
    <tbody><tr>
        <td width="140"><a href="imgs/nsvqa.jpg"><img src="imgs/nsvqa.jpg" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://papers.nips.cc/paper/7381-neural-symbolic-vqa-disentangling-reasoning-from-vision-and-language-understanding.pdf">  Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding</a> </strong></p>
          <p class="author"> Kexin Yi, Jiajun Wu, <strong>Chuang Gan</strong>, Antonio Torralba, Pushmeet Kohli, Joshua B. Tenenbaum </p>

          <p class="content">
          <p class="caption-2"> <strong> NIPS 2018  </strong>
          </tr>
    </tbody></table>
    </tbody></table>


    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/VQS.png"><img src="imgs/VQS.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Gan_VQS_Linking_Segmentations_ICCV_2017_paper.pdf"> VQS: Linking Segmentations to Questions and Answers for Supervised Attention in VQA and Question-Focused Semantic Segmentation</a> </strong></p>
          <p class="author"> <strong>Chuang Gan</strong>, Yandong Li, Haoxiang Li, Chen Sun, Boqing Gong</p>
          <p class="content">
          <p class="caption-2"> <strong> ICCV 2017 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <hr>

    <p class="title-small"> Visual Representations Learning</p>

      <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/Contrastive.png"><img src="imgs/Contrastive.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/2111.01124.pdf">When Does Contrastive Learning Preserve Adversarial Robustness from Pretraining to Finetuning? </a> </strong></p>
          <p class="author">  Lijie Fan, Sijia Liu, Pin-Yu Chen, Gaoyuan Zhang, <strong>Chuang Gan</strong>
    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2021 </strong>
          </tr>
    </tbody></table>
    </tbody></table>


       <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/video_grounding.jpeg"><img src="imgs/TinyTl.jpeg" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://hanlab.mit.edu/projects/tinyml/tinyTL"> TinyTL: Reduce Activations, Not Trainable Parameters for Efficient On-Device Learning
</a> </strong></p>
          <p class="author">  Han Cai, <strong>Chuang Gan</strong>, Ligeng Zhu, Song Han
    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2020 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

     <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/MCUNET.png"><img src="imgs/MCUNET.png" width="150" hight="200"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://mcunet.mit.edu/"> MCUNet: Tiny Deep Learning on IoT Devices

</a> </strong></p>
          <p class="author">  Ji Lin, Wei-Ming Chen, Yujun Lin, John Cohn, <strong>Chuang Gan</strong>, Song Han
    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2020 (Spotlight) </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/ofa.png"><img src="imgs/ofa.png" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://openreview.net/pdf?id=HylxE1HKwS">  Once for All: Train One Network and Specialize it for Efficient Deployment </a> </strong></p>
          <p class="author"> Han Cai, <strong>Chuang Gan</strong>, Tianzhe Wang, Zhekai Zhang, Song Han</p>

          <p class="content">
          <p class="caption-2"> <strong> ICLR 2020 </strong>
          </tr>
    </tbody></table>
    </tbody></table>


    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/ncn.png"><img src="imgs/ncn.png" width="150" hight="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://papers.nips.cc/paper/8411-cross-channel-communication-networks.pdf"> Cross-channel Communication Networks </a> </strong></p>
          <p class="author"> Jianwei Yang, Zhile Ren, <strong>Chuang Gan</strong>, Hongyuan Zhu, Devi Parikh

    </p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2019 </strong>
          </tr>
    </tbody></table>
    </tbody></table>


    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/tsm.png"><img src="imgs/tsm.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/1811.08383.pdf"> TSM: Temporal Shift Module for Efficient Video Understanding</a> </strong></p>
          <p class="author"> Ji Lin, <strong>Chuang Gan</strong>, Song Han </p>

          <p class="content">
          <p class="caption-2"> <strong> ICCV 2019  </strong>
          </tr>
    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/gcn_action.jpeg"><img src="imgs/gcn_action.jpeg" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/1909.03252.pdf"> Graph Convolutional Networks for Temporal Action Localization</a> </strong></p>
          <p class="author"> Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong, Peilin Zhao, Junzhou Huang, <strong>Chuang Gan</strong> </p>

          <p class="content">
          <p class="caption-2"> <strong> ICCV 2019  </strong>
          </tr>

    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/attention.png"><img src="imgs/attention.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Long_Attention_Clusters_Purely_CVPR_2018_paper.pdf">Attention Clusters: Purely Attention Based Local Feature Integration for Video Classification</a></strong>

          <p class="author"> Xiang Long, <strong>Chuang Gan</strong>, Gerard de Melo, Jiajun Wu, Xiao Liu, Shilei Wen</p>


          <p class="content">
          <p class="caption-2"> <strong> CVPR 2018  </strong>

          </tr>
    </tbody></table>


    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/tvnet.png"><img src="imgs/tvnet.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
        <p class="caption-1 "><strong><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Fan_End-to-End_Learning_of_CVPR_2018_paper.pdf">End-to-End Learning of Motion Representation for Video Understanding</a></strong>
        <p class="author"> Lijie Fan, Wenbing Huang, <strong>Chuang Gan</strong>, Stefano Ermon, Boqing Gong, Junzhou Huang</p>
          <p class="content">
          <p class="caption-2"> <strong> CVPR 2018 (Spotlight) </strong>

          </tr>
    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/devnet.png"><img src="imgs/devnet.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "> <strong><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Gan_DevNet_A_Deep_2015_CVPR_paper.pdf"> DevNet: A Deep Event Network for multimedia event detection and evidence recounting </a> </strong></p>
          <p class="author">  <strong>Chuang Gan</strong>, Naiyan Wang, Yi Yang, Dit-Yan Yeung, Alexander G. Hauptmann</p>


          <p class="content">
          <p class="caption-2"> <strong> CVPR 2015 </strong>

          </tr>
    </tbody></table>
    </tbody></table>
    </tbody></table>
    </tbody></table>



    <hr>

    <p class="title-small">Learning from Unlabeled Videos</p>



    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/geometry.png"><img src="imgs/geometry.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Gan_Geometry_Guided_Convolutional_CVPR_2018_paper.pdf">Geometry-Guided CNNs for Self-supervised Video Representation Learning</a></strong>

          <p class="author"><strong>Chuang Gan</strong>, Boqing Gong, Kun Liu, Hao Su, Leonidas Guibas</p>

          <p class="content">
          <p class="caption-2"> <strong> CVPR 2018  </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/lead_exceed.png"><img src="imgs/lead_exceed.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gan_You_Lead_We_CVPR_2016_paper.pdf">You Lead, We Exceed: Labor-Free Video Concept Learning by Jointly Exploiting Web Videos and Images</a> </strong></p>
          <p class="author"> <strong>Chuang Gan</strong>, Ting Yao, Kuiyuan Yang, Yi Yang, Tao Mei</p>
          <p class="content">
          <p class="caption-2"> <strong> CVPR 2016 (Spotlight) </strong>

          </tr>
    </tbody></table>
    </tbody></table>
    </tbody></table>


    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/zero-shot.png"><img src="imgs/zero-shot.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://rdcu.be/6su7"> Recognizing an Action Using Its Name: A Knowledge-Based Approach </a> </strong></p>
          <p class="author">  <strong>Chuang Gan</strong>, Yi Yang, Linchao Zhu, Deli Zhao, Yueting Zhuang</p>
          <p class="content">
          <p class="caption-2"> <strong> IJCV 2016 </strong>

          </tr>
    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/webly.png"><img src="imgs/webly.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://link.springer.com/chapter/10.1007/978-3-319-46487-9_52">Webly-Supervised Video Recognition by Mutually Voting for Relevant Web Images and Web Video Frames</a> </strong></p>
          <p class="author"> <strong>Chuang Gan</strong>, Chen Sun, Lixin Duan, Boqing Gong</p>
          <p class="content">
          <p class="caption-2"> <strong> ECCV 2016 </strong>

          </tr>
    </tbody></table>
    </tbody></table>


    <hr>
    <p class="title-small">Generative Models for Vision and Language</p>


    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/dense_caption.png"><img src="imgs/dense_caption.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://papers.nips.cc/paper/7569-weakly-supervised-dense-event-captioning-in-videos.pdf">  Weakly Supervised Dense Event Captioning in Videos</a> </strong></p>
          <p class="author"> Xuguang Duan, Wenbing Huang, <strong>Chuang Gan</strong>, Jingdong Wang, Wenwu Zhu,  Junzhou Huang</p>

          <p class="content">
          <p class="caption-2"> <strong> NeurIPS 2018 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/fated.png"><img src="imgs/fated.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "> <strong><a href="http://iiis.tsinghua.edu.cn/~weblt/papers/captioning-multi-faceted-attention.pdf"> Video Captioning with Multi-Faceted Attention</a> </strong></p>
          <p class="author">  Xiang Long, <strong>Chuang Gan</strong>, Gerard de Melo</p>
          <p class="content">
          <p class="caption-2"> <strong> TACL 2018 </strong>

          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/stylenet.jpg"><img src="imgs/stylenet.jpg" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Gan_StyleNet_Generating_Attractive_CVPR_2017_paper.pdf"> StyleNet: Generating Attractive Visual Captions with Styles</a> </strong></p>
          <p class="author">  <strong>Chuang Gan</strong>, Zhe Gan, Xiaodong He, Jianfeng Gao, Li Deng</p>
          <p class="content">
          <p class="caption-2"> <strong> CVPR 2017 </strong>
          </tr>
    </tbody></table>
    </tbody></table>

    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/SCN.png"><img src="imgs/SCN.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "> <strong><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Gan_Semantic_Compositional_Networks_CVPR_2017_paper.pdf"> Semantic Compositional Networks for Visual Captioning </a> </strong></p>
          <p class="author"> Zhe Gan, <strong>Chuang Gan</strong>, Xiaodong He, Yunchen Pu, Kenneth Tran, Jianfeng Gao, Lawrence Carin, Li Deng</p>
          <p class="content">
          <p class="caption-2"> <strong> CVPR 2017 (Spotlight) </strong>
          </tr>
    </tbody></table>
    </tbody></table>



    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/paragraph.png"><img src="imgs/paragraph.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Liang_Recurrent_Topic-Transition_GAN_ICCV_2017_paper.pdf">  Recurrent Topic-Transition GAN for Visual Paragraph Generation </a> </strong></p>
          <p class="author">  Xiaodan Liang, Zhiting Hu, Hao Zhang, <strong>Chuang Gan</strong>, Eric P. Xing</p>
          <p class="content">
          <p class="caption-2"> <strong> ICCV 2017 </strong>
          </tr>
    </tbody></table>
    </tbody></table>



    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/ACD.png"><img src="imgs/ACD.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/abs/1509.07225">Automatic Concept Discovery from Parallel Text and Visual Corpora</a> </strong></p>
          <p class="author"> Chen Sun, <strong>Chuang Gan</strong>, Ram Nevatia</p>
          <p class="content">
          <p class="caption-2"> <strong> ICCV 2015 </strong>

          </tr>

          </tr>
    </tbody></table>
    </tbody></table>


    </tbody></table>
    </tbody></table>
    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/contour.png"><img src="imgs/contour.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Dekel_Sparse_Smart_Contours_CVPR_2018_paper.pdf"> Sparse, Smart Contours to Represent and Edit Images </a></strong>
          <p class="author">  Tali Dekel, <strong>Chuang Gan</strong>, Dilip Krishnan, Ce Liu, William T. Freeman</p>


          <p class="content">
          <p class="caption-2"> <strong> CVPR 2018 </strong>

          </tr>
    </tbody></table>
    </tbody></table>

    <hr>
    <p class="title-small">Domaim Adaptation</p>

      <tbody><tr>
        <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/attribute.png"><img src="imgs/attribute.png" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gan_Learning_Attributes_Equals_CVPR_2016_paper.pdf"> Learning Attributes Equals Multi-Source Domain Generalization</a> </strong></p>
          <p class="author">  <strong>Chuang Gan</strong>, Tianbao Yang, Boqing Gong</p>
          <p class="content">
          <p class="caption-2"> <strong> CVPR 2016 (Spotlight)</strong>

          </tr>
    </tbody></table>


    <table border="0">
      <tbody><tr>
        <td width="140"><a href="imgs/keypoint.jpg"><img src="imgs/keypoint.jpg" width="150"></a></td>
        <td width="20"></td>
        <td valign="middle" width="800">
          <p class="caption-1 "><strong><a href="https://arxiv.org/pdf/1712.05765.pdf"> Unsupervised Domain Adaptation for 3D Keypoint Estimation via View Consistency </a> </strong></p>
          <p class="author"> Xingyi Zhou, Arjun Karpur,<strong>Chuang Gan</strong>, Linjie Luo, Qixing Huang</p>

          <p class="content">
          <p class="caption-2"> <strong> ECCV 2018  </strong>
          </tr>
    </tbody></table>
    </tbody></table>

      <!-- add more papers using above format -->
    </div>
    </div>

    <br>


    <!-- news here -->
    <p id="sect-competitions" class="title-large">Competitions</p>
    <p class="content">&bull; Rank 1st in ActivityNet AVA Challenge 2018</p>
    <p class="content">&bull; Rank 1st in ActivityNet Kinetics Challenge 2017</p>
    <p class="content">&bull; Rank 1st in NIST TRECVID MED and MER 2014<p>
    <p class="content">&bull; Rank 2nd in Moments in Time 2018</p>
    <p class="content">&bull; Rank 3rd in Youtube8M Challenge 2017</p>
    <p class="content">&bull; Rank 3rd in ActivityNet classification Challenge 2016</p>


    <!-- add more news as you like using above format -->
    <br>

    <!-- software here -->
    <p id="sect-software" class="title-large">Data &amp; Software</p>
    <p class="content">&bull; <strong><a href="https://github.com/kexinyi/ns-vqa">NS-VQA.</a></strong>  Neural-Symbolic Visual Reasoning.</p>
    <p class="content">&bull; <strong><a href="https://github.com/XgDuan/WSDEC">WSDEC.</a></strong>  Weakly-supervised Dense Event Captioning.</p>
    <p class="content">&bull; <strong><a href="http://sound-of-pixels.csail.mit.edu/">The Sound of Pixels.</a></strong>  Listen to the sound of pixels.</p>
    <p class="content">&bull; <strong><a href="https://contours2im.appspot.com/">Smart Contours.</a></strong>  Edit images using contours.</p>
    <p class="content">&bull; <strong><a href="https://github.com/longxiang92/Flash-MNIST">Attention Clusters.</a></strong>  Multiple and diverse attention for video classification.</p>
    <p class="content">&bull; <strong><a href="https://github.com/zhegan27/SCN_for_video_captioning">SCN.</a></strong>  Semantic composition network for image and video captioning.</p>
    <p class="content">&bull; <strong><a href="https://github.com/Cold-Winter/vqs">VQS.</a></strong> Visual question segmentation.</p>
    <p class="content">&bull; <strong><a href="https://github.com/LijieFan/tvnet">TVNET.</a></strong>  End to end video motion learning.</p>
    <p class="content">&bull; <strong><a href="https://github.com/chuangg/Youtube-8M">Youtube8M.</a></strong>  Temporal modeling for video classification.</p>

    <br>


    <!-- Awards here -->
    <p id="sect-Honors" class="title-large">Honors</p>
    <p class="content">&bull; Outstanding Doctoral Thesis Award at Tsinghua University (2018)</p>
    <p class="content">&bull; Excellent Graduate Student at Tsinghua University (2018)</p>
    <p class="content">&bull; Top Talented Graduate Student at Tsinghua University (2017)</p>
    <p class="content">&bull; Academic Rising Star Finalist at Tsinghua University (2016, 2017)</p>
    <p class="content">&bull; Microsoft Fellowship (2016)</p>
    <p class="content">&bull; Baidu Fellowship (2016)</p>
    <p class="content">&bull; National Scholarship, by Ministry of Education of China (2015)</p>

    <br>


    </html>
